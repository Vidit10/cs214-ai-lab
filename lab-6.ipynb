 import pandas as pd
 import numpy as np
 from sklearn.model_selection import train_test_split, GridSearchCV
 from sklearn.linear_model import LogisticRegression
 from sklearn.svm import SVC
 from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report
 import matplotlib.pyplot as plt
 import seaborn as sns
 

 ################################################################################
 # UTILITY FUNCTIONS
 ################################################################################
 

 def load_data(train_file, validation_file, test_file):
  """Loads data from CSV files into pandas DataFrames."""
  train_df = pd.read_csv(train_file)
  validation_df = pd.read_csv(validation_file)
  test_df = pd.read_csv(test_file)
  return train_df, validation_df, test_df
 

 def prepare_data(train_df, validation_df, test_df, target_column='Diagnosis'):
  """Prepares data for model training and evaluation.
 

  Separates features (X) and target (y).
  """
  X_train = train_df.drop(target_column, axis=1)
  y_train = train_df[target_column]
  X_validation = validation_df.drop(target_column, axis=1)
  y_validation = validation_df[target_column]
  X_test = test_df.drop(target_column, axis=1)
  y_test = test_df[target_column]
  return X_train, y_train, X_validation, y_validation, X_test, y_test
 

 def evaluate_model(model, X_validation, y_validation, X_test, y_test):
  """Evaluates the model on validation and test sets.
 

  Prints confusion matrix, accuracy, precision, recall, and F1-score.
  """
  print("Validation Data:")
  y_validation_pred = model.predict(X_validation)
  print("Confusion Matrix:\n", confusion_matrix(y_validation, y_validation_pred))
  print("Accuracy:", accuracy_score(y_validation, y_validation_pred))
  print("Precision:", precision_score(y_validation, y_validation_pred))
  print("Recall:", recall_score(y_validation, y_validation_pred))
  print("F1-score:", f1_score(y_validation, y_validation_pred))
  print(classification_report(y_validation, y_validation_pred))
 

  print("\nTest Data:")
  y_test_pred = model.predict(X_test)
  print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
  print("Accuracy:", accuracy_score(y_test, y_test_pred))
  print("Precision:", precision_score(y_test, y_test_pred))
  print("Recall:", recall_score(y_test, y_test_pred))
  print("F1-score:", f1_score(y_test, y_test_pred))
  print(classification_report(y_test, y_test_pred))
 

 def plot_weight_distribution(model, feature_names, title="Weight Distribution"):
  """Plots the distribution of feature weights."""
  if hasattr(model, 'coef_'):
  weights = model.coef_[0]
  else:
  weights = model.feature_importances_ # For models like RandomForest
  feature_weights = pd.DataFrame({'Feature': feature_names, 'Weight': weights})
  feature_weights['Absolute_Weight'] = np.abs(feature_weights['Weight'])
  feature_weights = feature_weights.sort_values('Absolute_Weight', ascending=False)
 

  plt.figure(figsize=(12, 6))
  sns.barplot(x='Weight', y='Feature', data=feature_weights.head(10), palette='viridis') # Showing top 10 features
  plt.title(title)
  plt.xlabel('Weight')
  plt.ylabel('Feature')
  plt.tight_layout()
  plt.show()
 

 ################################################################################
 # PART A: Logistic Regression on Original Data
 ################################################################################
 

 def part_a_logistic_regression():
  """Performs logistic regression on the original dataset."""
  print("Part A: Logistic Regression on Original Data\n")
 

  # 1. Load data
  train_file = "WDBC_Train.csv"
  validation_file = "WDBC_Validation.csv"
  test_file = "WDBC_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Implement Logistic Regression
  logistic_reg = LogisticRegression(max_iter=1000)  #increase max_iter to ensure convergence
  logistic_reg.fit(X_train, y_train)
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(logistic_reg, feature_names, title="Logistic Regression Weights on Original Data")
 

  # 5. Evaluate performance
  evaluate_model(logistic_reg, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART B: SVM with Linear Kernel on Original Data
 ################################################################################
 

 def part_b_svm_linear():
  """Performs SVM with a linear kernel on the original dataset."""
  print("\nPart B: SVM with Linear Kernel on Original Data\n")
 

  # 1. Load data
  train_file = "WDBC_Train.csv"
  validation_file = "WDBC_Validation.csv"
  test_file = "WDBC_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Linear Kernel and tune C using cross-validation
  param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
  grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_linear_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(best_linear_svm, feature_names, title="Linear SVM Weights on Original Data")
 

  # 5. Evaluate performance
  evaluate_model(best_linear_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART C: SVM with Gaussian (RBF) Kernel on Original Data
 ################################################################################
 

 def part_c_svm_rbf():
  """Performs SVM with a Gaussian (RBF) kernel on the original dataset."""
  print("\nPart C: SVM with Gaussian (RBF) Kernel on Original Data\n")
 

  # 1. Load data
  train_file = "WDBC_Train.csv"
  validation_file = "WDBC_Validation.csv"
  test_file = "WDBC_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with RBF Kernel and tune C and gamma using cross-validation
  param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 0.1, 1, 10]}
  grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_rbf_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
 

  # 4. Evaluate performance
  evaluate_model(best_rbf_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART D: SVM with Polynomial Kernel on Original Data
 ################################################################################
 

 def part_d_svm_poly():
  """Performs SVM with a Polynomial kernel on the original dataset."""
  print("\nPart D: SVM with Polynomial Kernel on Original Data\n")
 

  # 1. Load data
  train_file = "WDBC_Train.csv"
  validation_file = "WDBC_Validation.csv"
  test_file = "WDBC_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Polynomial Kernel and tune C, gamma and degree using cross-validation
  param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1], 'degree': [2, 3, 4]}
  grid_search = GridSearchCV(SVC(kernel='poly'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_poly_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
  print("Best degree parameter:", grid_search.best_params_['degree'])
 

  # 4. Evaluate performance
  evaluate_model(best_poly_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART E: Logistic Regression on Scaled Data
 ################################################################################
 

 def part_e_logistic_regression_scaled():
  """Performs logistic regression on the scaled dataset."""
  print("Part E: Logistic Regression on Scaled Data\n")
 

  # 1. Load data
  train_file = "WDBC_Scaled_Train.csv"
  validation_file = "WDBC_Scaled_Validation.csv"
  test_file = "WDBC_Scaled_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Implement Logistic Regression
  logistic_reg = LogisticRegression(max_iter=1000)  #increase max_iter to ensure convergence
  logistic_reg.fit(X_train, y_train)
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(logistic_reg, feature_names, title="Logistic Regression Weights on Scaled Data")
 

  # 5. Evaluate performance
  evaluate_model(logistic_reg, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART F: SVM with Linear Kernel on Scaled Data
 ################################################################################
 

 def part_f_svm_linear_scaled():
  """Performs SVM with a linear kernel on the scaled dataset."""
  print("\nPart F: SVM with Linear Kernel on Scaled Data\n")
 

  # 1. Load data
  train_file = "WDBC_Scaled_Train.csv"
  validation_file = "WDBC_Scaled_Validation.csv"
  test_file = "WDBC_Scaled_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Linear Kernel and tune C using cross-validation
  param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
  grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_linear_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(best_linear_svm, feature_names, title="Linear SVM Weights on Scaled Data")
 

  # 5. Evaluate performance
  evaluate_model(best_linear_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART G: SVM with Gaussian (RBF) Kernel on Scaled Data
 ################################################################################
 

 def part_g_svm_rbf_scaled():
  """Performs SVM with a Gaussian (RBF) kernel on the scaled dataset."""
  print("\nPart G: SVM with Gaussian (RBF) Kernel on Scaled Data\n")
 

  # 1. Load data
  train_file = "WDBC_Scaled_Train.csv"
  validation_file = "WDBC_Scaled_Validation.csv"
  test_file = "WDBC_Scaled_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with RBF Kernel and tune C and gamma using cross-validation
  param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 0.1, 1, 10]}
  grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_rbf_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
 

  # 4. Evaluate performance
  evaluate_model(best_rbf_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART H: SVM with Polynomial Kernel on Scaled Data
 ################################################################################
 

 def part_h_svm_poly_scaled():
  """Performs SVM with a Polynomial kernel on the scaled dataset."""
  print("\nPart H: SVM with Polynomial Kernel on Scaled Data\n")
 

  # 1. Load data
  train_file = "WDBC_Scaled_Train.csv"
  validation_file = "WDBC_Scaled_Validation.csv"
  test_file = "WDBC_Scaled_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Polynomial Kernel and tune C, gamma and degree using cross-validation
  param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1], 'degree': [2, 3, 4]}
  grid_search = GridSearchCV(SVC(kernel='poly'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_poly_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
  print("Best degree parameter:", grid_search.best_params_['degree'])
 

  # 4. Evaluate performance
  evaluate_model(best_poly_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART I: Logistic Regression on PCA Transformed Data
 ################################################################################
 

 def part_i_logistic_regression_pca():
  """Performs logistic regression on the PCA transformed dataset."""
  print("Part I: Logistic Regression on PCA Transformed Data\n")
 

  # 1. Load data
  train_file = "WDBC_PCA10_Train.csv"
  validation_file = "WDBC_PCA10_Validation.csv"
  test_file = "WDBC_PCA10_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Implement Logistic Regression
  logistic_reg = LogisticRegression(max_iter=1000)  #increase max_iter to ensure convergence
  logistic_reg.fit(X_train, y_train)
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(logistic_reg, feature_names, title="Logistic Regression Weights on PCA Data")
 

  # 5. Evaluate performance
  evaluate_model(logistic_reg, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART J: SVM with Linear Kernel on PCA Transformed Data
 ################################################################################
 

 def part_j_svm_linear_pca():
  """Performs SVM with a linear kernel on the PCA transformed dataset."""
  print("\nPart J: SVM with Linear Kernel on PCA Transformed Data\n")
 

  # 1. Load data
  train_file = "WDBC_PCA10_Train.csv"
  validation_file = "WDBC_PCA10_Validation.csv"
  test_file = "WDBC_PCA10_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Linear Kernel and tune C using cross-validation
  param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
  grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_linear_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
 

  # 4. Analyze feature importance and visualize weights
  feature_names = X_train.columns
  plot_weight_distribution(best_linear_svm, feature_names, title="Linear SVM Weights on PCA Data")
 

  # 5. Evaluate performance
  evaluate_model(best_linear_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART K: SVM with Gaussian (RBF) Kernel on PCA Transformed Data
 ################################################################################
 

 def part_k_svm_rbf_pca():
  """Performs SVM with a Gaussian (RBF) kernel on the PCA transformed dataset."""
  print("\nPart K: SVM with Gaussian (RBF) Kernel on PCA Transformed Data\n")
 

  # 1. Load data
  train_file = "WDBC_PCA10_Train.csv"
  validation_file = "WDBC_PCA10_Validation.csv"
  test_file = "WDBC_PCA10_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with RBF Kernel and tune C and gamma using cross-validation
  param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 0.1, 1, 10]}
  grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_rbf_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
 

  # 4. Evaluate performance
  evaluate_model(best_rbf_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART L: SVM with Polynomial Kernel on PCA Transformed Data
 ################################################################################
 

 def part_l_svm_poly_pca():
  """Performs SVM with a Polynomial kernel on the PCA transformed dataset."""
  print("\nPart L: SVM with Polynomial Kernel on PCA Transformed Data\n")
 

  # 1. Load data
  train_file = "WDBC_PCA10_Train.csv"
  validation_file = "WDBC_PCA10_Validation.csv"
  test_file = "WDBC_PCA10_Test.csv"
  train_df, validation_df, test_df = load_data(train_file, validation_file, test_file)
 

  # 2. Prepare data
  X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_data(train_df, validation_df, test_df)
 

  # 3. Train SVM with Polynomial Kernel and tune C, gamma and degree using cross-validation
  param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1], 'degree': [2, 3, 4]}
  grid_search = GridSearchCV(SVC(kernel='poly'), param_grid, cv=5, scoring='accuracy')
  grid_search.fit(X_train, y_train)
  best_poly_svm = grid_search.best_estimator_
  print("Best C parameter:", grid_search.best_params_['C'])
  print("Best gamma parameter:", grid_search.best_params_['gamma'])
  print("Best degree parameter:", grid_search.best_params_['degree'])
 

  # 4. Evaluate performance
  evaluate_model(best_poly_svm, X_validation, y_validation, X_test, y_test)
 

 ################################################################################
 # PART M: Comparison and Summarization of Accuracy
 ################################################################################
 

 def part_m_comparison():
  """Compares and summarizes the accuracy of all models."""
  print("\nPart M: Comparison and Summarization of Accuracy\n")
 

  # This part requires you to run all the previous parts and collect the accuracy
  # scores for each model on the validation and test sets.  I'll provide a
  # template for how to organize this data into a DataFrame for comparison.
 

  data = {
  "Dataset": ["Original", "Standardized", "PCA (l=10)"],
  "Logistic Regression": [None, None, None],
  "SVM Linear": [None, None, None],
  "SVM Gaussian": [None, None, None],
  "SVM Polynomial": [None, None, None]
  }
 

  df = pd.DataFrame(data)
  df.set_index("Dataset", inplace=True)
 

  # Populate the DataFrame with the accuracy scores you collected from previous parts
  # Example:
  # df.loc["Original", "Logistic Regression"] = 0.95  # Replace with actual accuracy
 

  print("Test Accuracy Comparison\n", df)
 

 ################################################################################
 # MAIN EXECUTION
 ################################################################################
 

 if __name__ == "__main__":
  part_a_logistic_regression()
  part_b_svm_linear()
  part_c_svm_rbf()
  part_d_svm_poly()
  part_e_logistic_regression_scaled()
  part_f_svm_linear_scaled()
  part_g_svm_rbf_scaled()
  part_h_svm_poly_scaled()
  part_i_logistic_regression_pca()
  part_j_svm_linear_pca()
  part_k_svm_rbf_pca()
  part_l_svm_poly_pca()
  part_m_comparison()
