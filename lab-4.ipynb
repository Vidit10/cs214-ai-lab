{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"CS214 Lab 4: Dimensionality Reduction and K-Nearest Neighbor Classifier.ipynb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mAutomatically generated by Colaboratory.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Part A: Data Preparation\n",
    "# 1. Load the WDBC dataset and display the first five rows.\n",
    "df = pd.read_csv(\"WisconsinDiagnosticBreastCancer.csv\")\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Drop the 'id' column, replace the 'diagnosis' column values with binary (M=1, B=0), and check for missing values.\n",
    "df = df.drop('id', axis=1)\n",
    "df['diagnosis'] = df['diagnosis'].replace({'M': 1, 'B': 0})\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Compute and visualize the correlation matrix using a heatmap.\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix of WDBC Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Split the dataset into training (60%), validation (20%), and testing (20%) sets for each class label (diagnosis).\n",
    "#    Save them as WDBC_Train.csv, WDBC_Validation.csv, and WDBC_Test.csv, respectively.\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_df.to_csv(\"WDBC_Train.csv\", index=False)\n",
    "valid_df.to_csv(\"WDBC_Validation.csv\", index=False)\n",
    "test_df.to_csv(\"WDBC_Test.csv\", index=False)\n",
    "\n",
    "print(\"\\nTraining, validation, and test datasets saved as CSV files.\")\n",
    "\n",
    "# Part B: Standardization\n",
    "# 1. Load the training data from CSV file (WDBC_Train.csv).\n",
    "train_df = pd.read_csv(\"WDBC_Train.csv\")\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "\n",
    "# 2. Calculate the mean and standard deviation of each training data attribute except the class attribute (diagnosis).\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "\n",
    "# 3. Standardize all the train data attributes except the class attribute. Save the standardized training data as WDBC_Scaled_Train.csv.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "train_scaled_df = pd.concat([X_train_scaled_df, y_train.reset_index(drop=True)], axis=1)\n",
    "train_scaled_df.to_csv(\"WDBC_Scaled_Train.csv\", index=False)\n",
    "\n",
    "# 4. Use the mean and standard deviation of training data to perform the standardization of validation and test data.\n",
    "#    Save the standardized validation and test data as WDBC_Scaled_Validation.csv and WDBC_Scaled_Test.csv, respectively.\n",
    "valid_df = pd.read_csv(\"WDBC_Validation.csv\")\n",
    "test_df = pd.read_csv(\"WDBC_Test.csv\")\n",
    "\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "valid_scaled_df = pd.concat([X_valid_scaled_df, y_valid.reset_index(drop=True)], axis=1)\n",
    "test_scaled_df = pd.concat([X_test_scaled_df, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "valid_scaled_df.to_csv(\"WDBC_Scaled_Validation.csv\", index=False)\n",
    "test_scaled_df.to_csv(\"WDBC_Scaled_Test.csv\", index=False)\n",
    "\n",
    "print(\"\\nStandardized training, validation, and test datasets saved as CSV files.\")\n",
    "\n",
    "# Part C: Principal Component Analysis (PCA)\n",
    "# 1. Load the training data from CSV file (WDBC_Train.csv).\n",
    "train_df = pd.read_csv(\"WDBC_Train.csv\")\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "\n",
    "valid_df = pd.read_csv(\"WDBC_Validation.csv\")\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "\n",
    "test_df = pd.read_csv(\"WDBC_Test.csv\")\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "# 2. Perform the PCA on all attributes except the class attribute.\n",
    "#    You need to fit and transform the PCA on the original training data. Then, observe the eigenvalues and corresponding eigenvectors.\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "# (a) Plot the eigenvalues in the descending order of their values.\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.title('PCA: Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# (b) Compute the covariance matrix of transformed data and observe its nature and variances.\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "covariance_matrix = np.cov(X_train_transformed.T)\n",
    "print(\"\\nCovariance Matrix of Transformed Data:\")\n",
    "print(covariance_matrix)\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "X_train_pca_2 = pca_2.fit_transform(X_train)\n",
    "X_valid_pca_2 = pca_2.transform(X_valid)\n",
    "X_test_pca_2 = pca_2.transform(X_test)\n",
    "\n",
    "train_pca_2_df = pd.concat([pd.DataFrame(X_train_pca_2), y_train.reset_index(drop=True)], axis=1)\n",
    "valid_pca_2_df = pd.concat([pd.DataFrame(X_valid_pca_2), y_valid.reset_index(drop=True)], axis=1)\n",
    "test_pca_2_df = pd.concat([pd.DataFrame(X_test_pca_2), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_pca_2_df.to_csv(\"WDBC_PCA2_Train.csv\", index=False)\n",
    "valid_pca_2_df.to_csv(\"WDBC_PCA2_Validation.csv\", index=False)\n",
    "test_pca_2_df.to_csv(\"WDBC_PCA2_Test.csv\", index=False)\n",
    "\n",
    "pca_10 = PCA(n_components=10)\n",
    "X_train_pca_10 = pca_10.fit_transform(X_train)\n",
    "X_valid_pca_10 = pca_10.transform(X_valid)\n",
    "X_test_pca_10 = pca_10.transform(X_test)\n",
    "\n",
    "train_pca_10_df = pd.concat([pd.DataFrame(X_train_pca_10), y_train.reset_index(drop=True)], axis=1)\n",
    "valid_pca_10_df = pd.concat([pd.DataFrame(X_valid_pca_10), y_valid.reset_index(drop=True)], axis=1)\n",
    "test_pca_10_df = pd.concat([pd.DataFrame(X_test_pca_10), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_pca_10_df.to_csv(\"WDBC_PCA10_Train.csv\", index=False)\n",
    "valid_pca_10_df.to_csv(\"WDBC_PCA10_Validation.csv\", index=False)\n",
    "test_pca_10_df.to_csv(\"WDBC_PCA10_Test.csv\", index=False)\n",
    "\n",
    "print(\"\\nPCA transformed training, validation, and test datasets saved as CSV files.\")\n",
    "\n",
    "# Part D: K-Nearest Neighbors (KNN) classification on original data\n",
    "# 1. Load train, validation, and test data from WDBC_Train.csv, WDBC_Validation.csv, and WDBC_Test.csv, respectively.\n",
    "train_df = pd.read_csv(\"WDBC_Train.csv\")\n",
    "valid_df = pd.read_csv(\"WDBC_Validation.csv\")\n",
    "test_df = pd.read_csv(\"WDBC_Test.csv\")\n",
    "\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "# 2. Implement KNN classification with K=1, 7, 11.\n",
    "k_values = [1, 7, 11]\n",
    "knn_classifiers = {}\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_classifiers[k] = knn\n",
    "\n",
    "# 3. Evaluate and compare the performance metrics on validation data.\n",
    "validation_results = {}\n",
    "for k, knn in knn_classifiers.items():\n",
    "    y_pred = knn.predict(X_valid)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "    validation_results[k] = {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    print(f\"\\nKNN with K={k} - Validation Data:\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "# 4. Plot the classification accuracy vs. K for validation data and choose the best K for the KNN classifier.\n",
    "accuracies = [validation_results[k]['accuracy'] for k in k_values]\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Accuracy vs. K Value (Validation Data)\")\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nBest K for KNN classifier: {best_k}\")\n",
    "\n",
    "# 5. Now, use the KNN classifier of best K to evaluate and compare the performance metrics on the test data.\n",
    "best_knn = knn_classifiers[best_k]\n",
    "y_pred_test = best_knn.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nKNN with Best K={best_k} - Test Data:\")\n",
    "print(\"Confusion Matrix:\\n\", cm_test)\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1-score:\", f1_test)\n",
    "\n",
    "# Part E: KNN classification on standardized data\n",
    "# 1. Load train, validation, and test data from WDBC_Scaled_Train.csv, WDBC_Scaled_Validation.csv, and WDBC_Scaled_Test.csv, respectively.\n",
    "train_df = pd.read_csv(\"WDBC_Scaled_Train.csv\")\n",
    "valid_df = pd.read_csv(\"WDBC_Scaled_Validation.csv\")\n",
    "test_df = pd.read_csv(\"WDBC_Scaled_Test.csv\")\n",
    "\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "# 2. Implement KNN classification with K=1, 7, 11.\n",
    "k_values = [1, 7, 11]\n",
    "knn_classifiers = {}\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_classifiers[k] = knn\n",
    "\n",
    "# 3. Evaluate and compare the performance metrics on validation data.\n",
    "validation_results = {}\n",
    "for k, knn in knn_classifiers.items():\n",
    "    y_pred = knn.predict(X_valid)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "    validation_results[k] = {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    print(f\"\\nKNN with K={k} - Validation Data:\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "# 4. Plot the classification accuracy vs. K for validation data and choose the best K for the KNN classifier.\n",
    "accuracies = [validation_results[k]['accuracy'] for k in k_values]\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Accuracy vs. K Value (Validation Data - Standardized Data)\")\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nBest K for KNN classifier (Standardized Data): {best_k}\")\n",
    "\n",
    "# 5. Now, use the KNN classifier of best K to evaluate and compare the performance metrics on the test data.\n",
    "best_knn = knn_classifiers[best_k]\n",
    "y_pred_test = best_knn.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nKNN with Best K={best_k} - Test Data (Standardized Data):\")\n",
    "print(\"Confusion Matrix:\\n\", cm_test)\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1-score:\", f1_test)\n",
    "\n",
    "# Part F: KNN classification on PCA (l=2) transformed data\n",
    "# 1. Load train, validation, and test data from WDBC_PCA2_Train.csv, WDBC_PCA2_Validation.csv, and WDBC_PCA2_Test.csv, respectively.\n",
    "train_df = pd.read_csv(\"WDBC_PCA2_Train.csv\")\n",
    "valid_df = pd.read_csv(\"WDBC_PCA2_Validation.csv\")\n",
    "test_df = pd.read_csv(\"WDBC_PCA2_Test.csv\")\n",
    "\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "# 2. Implement KNN classification with K=1, 7, 11.\n",
    "k_values = [1, 7, 11]\n",
    "knn_classifiers = {}\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_classifiers[k] = knn\n",
    "\n",
    "# 3. Evaluate and compare the performance metrics on validation data.\n",
    "validation_results = {}\n",
    "for k, knn in knn_classifiers.items():\n",
    "    y_pred = knn.predict(X_valid)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "    validation_results[k] = {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    print(f\"\\nKNN with K={k} - Validation Data (PCA l=2):\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "# 4. Plot the classification accuracy vs. K for validation data and choose the best K for the KNN classifier.\n",
    "accuracies = [validation_results[k]['accuracy'] for k in k_values]\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Accuracy vs. K Value (Validation Data - PCA l=2)\")\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nBest K for KNN classifier (PCA l=2): {best_k}\")\n",
    "\n",
    "# 5. Now, use the KNN classifier of best K to evaluate and compare the performance metrics on the test data.\n",
    "best_knn = knn_classifiers[best_k]\n",
    "y_pred_test = best_knn.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nKNN with Best K={best_k} - Test Data (PCA l=2):\")\n",
    "print(\"Confusion Matrix:\\n\", cm_test)\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1-score:\", f1_test)\n",
    "\n",
    "# Part G: KNN classification on PCA (l=10) transformed data\n",
    "# 1. Load train, validation, and test data from WDBC_PCA10_Train.csv, WDBC_PCA10_Validation.csv, and WDBC_PCA10_Test.csv, respectively.\n",
    "train_df = pd.read_csv(\"WDBC_PCA10_Train.csv\")\n",
    "valid_df = pd.read_csv(\"WDBC_PCA10_Validation.csv\")\n",
    "test_df = pd.read_csv(\"WDBC_PCA10_Test.csv\")\n",
    "\n",
    "X_train = train_df.drop('diagnosis', axis=1)\n",
    "y_train = train_df['diagnosis']\n",
    "X_valid = valid_df.drop('diagnosis', axis=1)\n",
    "y_valid = valid_df['diagnosis']\n",
    "X_test = test_df.drop('diagnosis', axis=1)\n",
    "y_test = test_df['diagnosis']\n",
    "\n",
    "# 2. Implement KNN classification with K=1, 7, 11.\n",
    "k_values = [1, 7, 11]\n",
    "knn_classifiers = {}\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_classifiers[k] = knn\n",
    "\n",
    "# 3. Evaluate and compare the performance metrics on validation data.\n",
    "validation_results = {}\n",
    "for k, knn in knn_classifiers.items():\n",
    "    y_pred = knn.predict(X_valid)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "\n",
    "    validation_results[k] = {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    print(f\"\\nKNN with K={k} - Validation Data (PCA l=10):\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "# 4. Plot the classification accuracy vs. K for validation data and choose the best K for the KNN classifier.\n",
    "accuracies = [validation_results[k]['accuracy'] for k in k_values]\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"KNN Accuracy vs. K Value (Validation Data - PCA l=10)\")\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nBest K for KNN classifier (PCA l=10): {best_k}\")\n",
    "\n",
    "# 5. Now, use the KNN classifier of best K to evaluate and compare the performance metrics on the test data.\n",
    "best_knn = knn_classifiers[best_k]\n",
    "y_pred_test = best_knn.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nKNN with Best K={best_k} - Test Data (PCA l=10):\")\n",
    "print(\"Confusion Matrix:\\n\", cm_test)\n",
    "print(\"Accuracy:\", accuracy_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
